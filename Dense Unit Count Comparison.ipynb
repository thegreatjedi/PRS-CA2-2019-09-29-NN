{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Dense Layer Unit Count Comparison\n",
    "\n",
    "In standard VGG-Net architectures, the network consists of two hidden \n",
    "dense layers consisting of 4096 units each. This notebook will explore\n",
    "different values to find the best performing value for our model.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gc import collect\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "from h5py import File\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from tensorflow import ConfigProto, Session\n",
    "from tensorflow.keras.backend import set_session, clear_session\n",
    "from tensorflow.keras.callbacks import CSVLogger, LearningRateScheduler, \\\n",
    "    ModelCheckpoint\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Input, MaxPool2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.random import set_random_seed\n",
    "from pandas import read_csv\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Improve GPU memory utilisation\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.log_device_placement = True\n",
    "sess = Session(config=config)\n",
    "set_session(sess)\n",
    "\n",
    "# Fix tensorflow random seed\n",
    "set_random_seed(324)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Creation Function\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_model(input_shape: Tuple[int, int, int], num_classes: int,\n",
    "                 num_dense_units: int = 4096) -> Model:\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    layer = Conv2D(filters=64, kernel_size=(3, 3), strides=1, padding=\"same\", \n",
    "                   activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                   bias_initializer=\"he_normal\")(inputs)\n",
    "    layer = Conv2D(filters=64, kernel_size=(3, 3), strides=1, padding=\"same\", \n",
    "                   activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = MaxPool2D(pool_size=(2, 2), strides=2)(layer)\n",
    "\n",
    "    layer = Conv2D(filters=128, kernel_size=(3, 3), strides=1, padding=\"same\", \n",
    "                   activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = Conv2D(filters=128, kernel_size=(3, 3), strides=1, padding=\"same\", \n",
    "                   activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = MaxPool2D(pool_size=(2, 2), strides=2)(layer)\n",
    "    \n",
    "    layer = Conv2D(filters=256, kernel_size=(3, 3), strides=1, padding=\"same\", \n",
    "                   activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = Conv2D(filters=256, kernel_size=(3, 3), strides=1, padding=\"same\", \n",
    "                   activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = Conv2D(filters=256, kernel_size=(1, 1), strides=1, padding=\"same\", \n",
    "                   activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = MaxPool2D(pool_size=(2, 2), strides=2)(layer)\n",
    "    \n",
    "    layer = Conv2D(filters=512, kernel_size=(3, 3), strides=1, padding=\"same\", \n",
    "                   activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = Conv2D(filters=512, kernel_size=(3, 3), strides=1, padding=\"same\", \n",
    "                   activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = Conv2D(filters=512, kernel_size=(1, 1), strides=1, padding=\"same\", \n",
    "                   activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = MaxPool2D(pool_size=(2, 2), strides=2)(layer)\n",
    "    \n",
    "    layer = Conv2D(filters=512, kernel_size=(3, 3), strides=1, padding=\"same\", \n",
    "                   activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = Conv2D(filters=512, kernel_size=(3, 3), strides=1, padding=\"same\", \n",
    "                   activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = Conv2D(filters=512, kernel_size=(1, 1), strides=1, padding=\"same\", \n",
    "                   activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = MaxPool2D(pool_size=(2, 2), strides=2)(layer)\n",
    "    \n",
    "    layer = Flatten()(layer)\n",
    "    layer = Dense(units=num_dense_units, activation=\"relu\", \n",
    "                  kernel_initializer=\"he_normal\",\n",
    "                  bias_initializer=\"he_normal\")(layer)\n",
    "    layer = Dense(units=num_dense_units, activation=\"relu\", \n",
    "                  kernel_initializer=\"he_normal\",\n",
    "                  bias_initializer=\"he_normal\")(layer)\n",
    "    layer = Dense(num_classes, activation=\"softmax\")(layer)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=layer)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), \n",
    "                  loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Other Functions\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def import_dataset(filepath: str = \"./dataset.hdf5\") \\\n",
    "        -> Tuple[np.ndarray, np.ndarray, np.ndarray, \n",
    "                 np.ndarray, np.ndarray, np.ndarray]:\n",
    "    file = File(filepath, \"r\")\n",
    "    train_data = file.get(\"tr_data\")[()]\n",
    "    val_data = file.get(\"val_data\")[()]\n",
    "    test_data = file.get(\"ts_data\")[()]\n",
    "    train_labels = file.get(\"tr_labels\")[()]\n",
    "    val_labels = file.get(\"val_labels\")[()]\n",
    "    test_labels = file.get(\"ts_labels\")[()]\n",
    "    \n",
    "    return train_data, val_data, test_data, \\\n",
    "           train_labels, val_labels, test_labels\n",
    "\n",
    "\n",
    "def get_test_results(test_model: Model, test_data: np.ndarray, \n",
    "                     test_labels: np.ndarray) -> Tuple:\n",
    "    predicts = test_model.predict(test_data)\n",
    "    pred_out = np.argmax(predicts, axis=1)\n",
    "    test_out = np.argmax(test_labels, axis=1)\n",
    "    labels = [\"car\", \"heavy vehicles\", \"motorcycle\"]\n",
    "    \n",
    "    return accuracy_score(test_out, pred_out), \\\n",
    "           confusion_matrix(test_out, pred_out), \\\n",
    "           classification_report(test_out, pred_out, target_names=labels)\n",
    "\n",
    "\n",
    "def get_learn_rate(epoch: int) -> float:\n",
    "    lr = 1e-4\n",
    "    if epoch > 10:\n",
    "        lr = 1e-5\n",
    "    elif epoch > 20:\n",
    "        lr = 1e-6\n",
    "    elif epoch > 30:\n",
    "        lr = 1e-7\n",
    "    elif epoch > 40:\n",
    "        lr = 1e-8\n",
    "    elif epoch > 50:\n",
    "        lr = 1e-9\n",
    "        \n",
    "    print(f\"Learning rate: {lr}\")\n",
    "    \n",
    "    return lr\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Initialisation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tr_dat, val_dat, ts_dat, tr_lbls, val_lbls, ts_lbls = import_dataset()\n",
    "\n",
    "in_shape = (tr_dat.shape[1], tr_dat.shape[2], tr_dat.shape[3])\n",
    "num_cls = tr_lbls.shape[1]\n",
    "\n",
    "model = None\n",
    "lr_scheduler = LearningRateScheduler(get_learn_rate)\n",
    "\n",
    "# Clear test data from memory as we're not using it during fine-tuning\n",
    "del ts_dat\n",
    "del ts_lbls\n",
    "collect()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Creation and Training\n",
    "\n",
    "We compare between different powers of 2 for computational efficiency, \n",
    "from 128 up to 4096."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 128 dense units\n",
    "model_dense_units = 128\n",
    "model = create_model(in_shape, num_cls, model_dense_units)\n",
    "print(model.summary())\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"./trained_models/128denseunits_best.hdf5\", \n",
    "                             monitor=\"val_loss\", verbose=0, \n",
    "                             save_best_only=True, mode=\"min\")\n",
    "logger = CSVLogger(\"./training_logs/128denseunits_log.csv\")\n",
    "\n",
    "model.fit(tr_dat, tr_lbls, batch_size=32, validation_data=(val_dat, val_lbls), \n",
    "          epochs=20, verbose=2, shuffle=True, \n",
    "          callbacks=[checkpoint, logger, lr_scheduler])\n",
    "\n",
    "model.save(f\"./trained_models/128denseunits_20epoch.hdf5\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 256 dense units\n",
    "model_dense_units = 256\n",
    "model = create_model(in_shape, num_cls, model_dense_units)\n",
    "print(model.summary())\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"./trained_models/256denseunits_best.hdf5\", \n",
    "                             monitor=\"val_loss\", verbose=0, \n",
    "                             save_best_only=True, mode=\"min\")\n",
    "logger = CSVLogger(\"./training_logs/256denseunits_log.csv\")\n",
    "\n",
    "model.fit(tr_dat, tr_lbls, batch_size=32, validation_data=(val_dat, val_lbls), \n",
    "          epochs=20, verbose=2, shuffle=True, \n",
    "          callbacks=[checkpoint, logger, lr_scheduler])\n",
    "\n",
    "model.save(f\"./trained_models/256denseunits_20epoch.hdf5\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 512 dense units\n",
    "model_dense_units = 512\n",
    "model = create_model(in_shape, num_cls, model_dense_units)\n",
    "print(model.summary())\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"./trained_models/512denseunits_best.hdf5\", \n",
    "                             monitor=\"val_loss\", verbose=0, \n",
    "                             save_best_only=True, mode=\"min\")\n",
    "logger = CSVLogger(\"./training_logs/512denseunits_log.csv\")\n",
    "\n",
    "model.fit(tr_dat, tr_lbls, batch_size=32, validation_data=(val_dat, val_lbls), \n",
    "          epochs=20, verbose=2, shuffle=True, \n",
    "          callbacks=[checkpoint, logger, lr_scheduler])\n",
    "\n",
    "model.save(f\"./trained_models/512denseunits_20epoch.hdf5\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1024 dense units\n",
    "model_dense_units = 1024\n",
    "model = create_model(in_shape, num_cls, model_dense_units)\n",
    "print(model.summary())\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"./trained_models/1024denseunits_best.hdf5\", \n",
    "                             monitor=\"val_loss\", verbose=0, \n",
    "                             save_best_only=True, mode=\"min\")\n",
    "logger = CSVLogger(\"./training_logs/1024denseunits_log.csv\")\n",
    "\n",
    "model.fit(tr_dat, tr_lbls, batch_size=32, validation_data=(val_dat, val_lbls), \n",
    "          epochs=20, verbose=2, shuffle=True, \n",
    "          callbacks=[checkpoint, logger, lr_scheduler])\n",
    "\n",
    "model.save(f\"./trained_models/1024denseunits_20epoch.hdf5\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2048 dense units\n",
    "model_dense_units = 2048\n",
    "model = create_model(in_shape, num_cls, model_dense_units)\n",
    "print(model.summary())\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"./trained_models/2048denseunits_best.hdf5\", \n",
    "                             monitor=\"val_loss\", verbose=0, \n",
    "                             save_best_only=True, mode=\"min\")\n",
    "logger = CSVLogger(\"./training_logs/2048denseunits_log.csv\")\n",
    "\n",
    "model.fit(tr_dat, tr_lbls, batch_size=32, validation_data=(val_dat, val_lbls), \n",
    "          epochs=20, verbose=2, shuffle=True, \n",
    "          callbacks=[checkpoint, logger, lr_scheduler])\n",
    "\n",
    "model.save(f\"./trained_models/2048denseunits_20epoch.hdf5\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training stage complete: free training data memory\n",
    "del tr_dat\n",
    "del tr_lbls\n",
    "collect()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Visualisation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "acc_scores = dict()\n",
    "conf_matrices = dict()\n",
    "class_reports = dict()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 128 dense units\n",
    "model_128 = create_model(in_shape, num_cls, 128)\n",
    "model_128.load_weights(\"./trained_models/128denseunits_best.hdf5\")\n",
    "model_128.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "                  loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "acc_scores[128], conf_matrices[128], class_reports[128] \\\n",
    "    = get_test_results(model_128, val_dat, val_lbls)\n",
    "\n",
    "print(f\"Validation accuracy for 128 dense units: {acc_scores[128]}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrices[128])\n",
    "print(class_reports[128])\n",
    "\n",
    "dense128_log = read_csv(\"./training_logs/128denseunits_log.csv\")\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=[10, 7.5])\n",
    "plt.subplot(211)\n",
    "plt.xticks([0, 5, 10, 15, 20])\n",
    "plt.plot(dense128_log[\"loss\"], label=\"train loss\")\n",
    "plt.plot(dense128_log[\"val_loss\"], label=\"validation loss\")\n",
    "plt.title(\"Loss\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.xticks([0, 5, 10, 15, 20])\n",
    "plt.plot(dense128_log[\"acc\"], label=\"train accuracy\")\n",
    "plt.plot(dense128_log[\"val_acc\"], label=\"validation accuracy\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n",
    "\n",
    "# Free memory\n",
    "clear_session()\n",
    "collect()\n",
    "del dense128_log\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 256 dense units\n",
    "model_256 = create_model(in_shape, num_cls, 256)\n",
    "model_256.load_weights(\"./trained_models/256denseunits_best.hdf5\")\n",
    "model_256.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "                  loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "acc_scores[256], conf_matrices[256], class_reports[256] \\\n",
    "    = get_test_results(model_256, val_dat, val_lbls)\n",
    "\n",
    "print(f\"Validation accuracy for 256 dense units: {acc_scores[256]}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrices[256])\n",
    "print(class_reports[256])\n",
    "\n",
    "dense256_log = read_csv(\"./training_logs/256denseunits_log.csv\")\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=[10, 7.5])\n",
    "plt.subplot(211)\n",
    "plt.xticks([0, 5, 10, 15, 20])\n",
    "plt.plot(dense256_log[\"loss\"], label=\"train loss\")\n",
    "plt.plot(dense256_log[\"val_loss\"], label=\"validation loss\")\n",
    "plt.title(\"Loss\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.xticks([0, 5, 10, 15, 20])\n",
    "plt.plot(dense256_log[\"acc\"], label=\"train accuracy\")\n",
    "plt.plot(dense256_log[\"val_acc\"], label=\"validation accuracy\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n",
    "\n",
    "# Free memory\n",
    "clear_session()\n",
    "collect()\n",
    "del dense256_log\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 512 dense units\n",
    "model_512 = create_model(in_shape, num_cls, 512)\n",
    "model_512.load_weights(\"./trained_models/512denseunits_best.hdf5\")\n",
    "model_512.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "                  loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "acc_scores[512], conf_matrices[512], class_reports[512] \\\n",
    "    = get_test_results(model_512, val_dat, val_lbls)\n",
    "\n",
    "print(f\"Validation accuracy for 512 dense units: {acc_scores[512]}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrices[512])\n",
    "print(class_reports[512])\n",
    "\n",
    "dense512_log = read_csv(\"./training_logs/512denseunits_log.csv\")\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=[10, 7.5])\n",
    "plt.subplot(211)\n",
    "plt.xticks([0, 5, 10, 15, 20])\n",
    "plt.plot(dense512_log[\"loss\"], label=\"train loss\")\n",
    "plt.plot(dense512_log[\"val_loss\"], label=\"validation loss\")\n",
    "plt.title(\"Loss\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.xticks([0, 5, 10, 15, 20])\n",
    "plt.plot(dense512_log[\"acc\"], label=\"train accuracy\")\n",
    "plt.plot(dense512_log[\"val_acc\"], label=\"validation accuracy\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n",
    "\n",
    "# Free memory\n",
    "clear_session()\n",
    "collect()\n",
    "del dense512_log\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1024 dense units\n",
    "model_1024 = create_model(in_shape, num_cls, 1024)\n",
    "model_1024.load_weights(\"./trained_models/1024denseunits_best.hdf5\")\n",
    "model_1024.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "                  loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "acc_scores[1024], conf_matrices[1024], class_reports[1024] \\\n",
    "    = get_test_results(model_1024, val_dat, val_lbls)\n",
    "\n",
    "print(f\"Validation accuracy for 1024 dense units: {acc_scores[1024]}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrices[1024])\n",
    "print(class_reports[1024])\n",
    "\n",
    "dense1024_log = read_csv(\"./training_logs/1024denseunits_log.csv\")\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=[10, 7.5])\n",
    "plt.subplot(211)\n",
    "plt.xticks([0, 5, 10, 15, 20])\n",
    "plt.plot(dense1024_log[\"loss\"], label=\"train loss\")\n",
    "plt.plot(dense1024_log[\"val_loss\"], label=\"validation loss\")\n",
    "plt.title(\"Loss\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.xticks([0, 5, 10, 15, 20])\n",
    "plt.plot(dense1024_log[\"acc\"], label=\"train accuracy\")\n",
    "plt.plot(dense1024_log[\"val_acc\"], label=\"validation accuracy\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n",
    "\n",
    "# Free memory\n",
    "clear_session()\n",
    "collect()\n",
    "del dense1024_log\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2048 dense units\n",
    "model_2048 = create_model(in_shape, num_cls, 2048)\n",
    "model_2048.load_weights(\"./trained_models/2048denseunits_best.hdf5\")\n",
    "model_2048.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "                  loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "acc_scores[2048], conf_matrices[2048], class_reports[2048] \\\n",
    "    = get_test_results(model_2048, val_dat, val_lbls)\n",
    "\n",
    "print(f\"Validation accuracy for 2048 dense units: {acc_scores[2048]}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrices[2048])\n",
    "print(class_reports[2048])\n",
    "\n",
    "dense2048_log = read_csv(\"./training_logs/2048denseunits_log.csv\")\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=[10, 7.5])\n",
    "plt.subplot(211)\n",
    "plt.xticks([0, 5, 10, 15, 20])\n",
    "plt.plot(dense2048_log[\"loss\"], label=\"train loss\")\n",
    "plt.plot(dense2048_log[\"val_loss\"], label=\"validation loss\")\n",
    "plt.title(\"Loss\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.xticks([0, 5, 10, 15, 20])\n",
    "plt.plot(dense2048_log[\"acc\"], label=\"train accuracy\")\n",
    "plt.plot(dense2048_log[\"val_acc\"], label=\"validation accuracy\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n",
    "\n",
    "# Free memory\n",
    "clear_session()\n",
    "collect()\n",
    "del dense2048_log\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 4096 dense units\n",
    "# Reuse existing model\n",
    "model_4096 = create_model(in_shape, num_cls, 4096)\n",
    "model_4096.load_weights(\"./trained_models/vgg16conv1_best.hdf5\")\n",
    "model_4096.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "                  loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "acc_scores[4096], conf_matrices[4096], class_reports[4096] \\\n",
    "    = get_test_results(model_4096, val_dat, val_lbls)\n",
    "\n",
    "print(f\"Validation accuracy for 4096 dense units: {acc_scores[4096]}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrices[4096])\n",
    "print(class_reports[4096])\n",
    "\n",
    "dense4096_log = read_csv(\"./training_logs/vgg16conv1_log.csv\")\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=[10, 7.5])\n",
    "plt.subplot(211)\n",
    "plt.xticks([0, 5, 10, 15, 20])\n",
    "plt.plot(dense4096_log[\"loss\"], label=\"train loss\")\n",
    "plt.plot(dense4096_log[\"val_loss\"], label=\"validation loss\")\n",
    "plt.title(\"Loss\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.xticks([0, 5, 10, 15, 20])\n",
    "plt.plot(dense4096_log[\"acc\"], label=\"train accuracy\")\n",
    "plt.plot(dense4096_log[\"val_acc\"], label=\"validation accuracy\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n",
    "\n",
    "# Free memory\n",
    "clear_session()\n",
    "collect()\n",
    "del dense4096_log\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Cross-Model Comparison"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=[10, 7.5])\n",
    "plt.subplot(211)\n",
    "plt.xticks([0, 5, 10, 15, 20])\n",
    "plt.plot(dense128_log[\"val_loss\"], label=\"128 units\")\n",
    "plt.plot(dense256_log[\"val_loss\"], label=\"256 units\")\n",
    "plt.plot(dense512_log[\"val_loss\"], label=\"512 units\")\n",
    "plt.plot(dense1024_log[\"val_loss\"], label=\"1024 units\")\n",
    "plt.plot(dense2048_log[\"val_loss\"], label=\"2048 units\")\n",
    "plt.title(\"Validation Loss\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.xticks([0, 5, 10, 15, 20])\n",
    "plt.plot(dense128_log[\"val_acc\"], label=\"128 units\")\n",
    "plt.plot(dense256_log[\"val_acc\"], label=\"256 units\")\n",
    "plt.plot(dense512_log[\"val_acc\"], label=\"512 units\")\n",
    "plt.plot(dense1024_log[\"val_acc\"], label=\"1024 units\")\n",
    "plt.plot(dense2048_log[\"val_acc\"], label=\"2048 units\")\n",
    "plt.title(\"Validation Accuracy\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Conclusion\n",
    "\n",
    "# TO BE UPDATED\n",
    "\n",
    "From the above, we can observe that overfitting occurs in all models \n",
    "between the 5-10th epoch, and maximum validation accuracy is about 65%. \n",
    "However, this is before any regularisation measures are taken.\n",
    "\n",
    "Based on the cross-model analysis comparisons, it can be seen that \n",
    "VGG-16 and VGG-16(Conv1) achieved the lowest validation loss, and at \n",
    "those points, VGG-16(Conv1) achieved the higher validation accuracy.\n",
    "\n",
    "Therefore, we shall proceed to optimise the VGG-16(Conv1) model with\n",
    "hyperparameter tuning, followed by regularisation."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
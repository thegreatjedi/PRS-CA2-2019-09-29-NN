{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Pooling Layer Window Size Comparison\n",
    "\n",
    "In standard VGG-Net architectures, Only 2x2 pooling windows are used. This \n",
    "notebook will explore whether other window sizes would give our model\n",
    "better performance.\n",
    "\n",
    "We shall look at 2x2, 3x3, and 5x5 window sizes. For simplicity, all\n",
    "pooling layers shall maintain the same window size throughout the\n",
    "model, and the window shall have the same length in both dimensions.\n",
    "Also, the stride length will also follow the window size.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gc import collect\n",
    "from typing import Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from h5py import File\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import accuracy_score, classification_report, \\\n",
    "    confusion_matrix\n",
    "from tensorflow.config.experimental import list_physical_devices, \\\n",
    "    set_memory_growth\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.callbacks import CSVLogger, LearningRateScheduler, \\\n",
    "    ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Input, MaxPool2D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.random import set_random_seed\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Creation Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_model(input_shape: Tuple[int, int, int], num_classes: int, \n",
    "                 window_size: int = 3) -> Model:\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    layer = Conv2D(filters=64, kernel_size=(3, 3), strides=1, \n",
    "                   padding=\"same\", activation=\"relu\", \n",
    "                   kernel_initializer=\"he_normal\", \n",
    "                   bias_initializer=\"he_normal\")(inputs)\n",
    "    layer = Conv2D(filters=64, kernel_size=(3, 3), strides=1, \n",
    "                   padding=\"same\", activation=\"relu\", \n",
    "                   kernel_initializer=\"he_normal\", \n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = MaxPool2D(pool_size=window_size, strides=window_size)(layer)\n",
    "\n",
    "    layer = Conv2D(filters=128, kernel_size=(3, 3), strides=1, \n",
    "                   padding=\"same\", activation=\"relu\", \n",
    "                   kernel_initializer=\"he_normal\", \n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = Conv2D(filters=128, kernel_size=(3, 3), strides=1, \n",
    "                   padding=\"same\", activation=\"relu\", \n",
    "                   kernel_initializer=\"he_normal\", \n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = MaxPool2D(pool_size=window_size, strides=window_size)(layer)\n",
    "    \n",
    "    layer = Conv2D(filters=256, kernel_size=(3, 3), strides=1, \n",
    "                   padding=\"same\", activation=\"relu\", \n",
    "                   kernel_initializer=\"he_normal\", \n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = Conv2D(filters=256, kernel_size=(3, 3), strides=1, \n",
    "                   padding=\"same\", activation=\"relu\", \n",
    "                   kernel_initializer=\"he_normal\", \n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = MaxPool2D(pool_size=window_size, strides=window_size)(layer)\n",
    "    \n",
    "    layer = Conv2D(filters=512, kernel_size=(3, 3), strides=1, \n",
    "                   padding=\"same\", activation=\"relu\", \n",
    "                   kernel_initializer=\"he_normal\", \n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = Conv2D(filters=512, kernel_size=(3, 3), strides=1, \n",
    "                   padding=\"same\", activation=\"relu\", \n",
    "                   kernel_initializer=\"he_normal\", \n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = MaxPool2D(pool_size=window_size, strides=window_size)(layer)\n",
    "    \n",
    "    layer = Conv2D(filters=512, kernel_size=(3, 3), strides=1, \n",
    "                   padding=\"same\", activation=\"relu\", \n",
    "                   kernel_initializer=\"he_normal\", \n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = Conv2D(filters=512, kernel_size=(3, 3), strides=1, \n",
    "                   padding=\"same\", activation=\"relu\", \n",
    "                   kernel_initializer=\"he_normal\", \n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = MaxPool2D(pool_size=window_size, strides=window_size)(layer)\n",
    "    \n",
    "    layer = Flatten()(layer)\n",
    "    layer = Dense(units=4096, activation=\"relu\", \n",
    "                  kernel_initializer=\"he_normal\",\n",
    "                  bias_initializer=\"he_normal\")(layer)\n",
    "    layer = Dense(units=4096, activation=\"relu\", \n",
    "                  kernel_initializer=\"he_normal\",\n",
    "                  bias_initializer=\"he_normal\")(layer)\n",
    "    layer = Dense(num_classes, activation=\"softmax\")(layer)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=layer)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), \n",
    "                  loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Other Functions\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def refresh_session():\n",
    "    # Call this before training a new model, to free up memory from the \n",
    "    # previous model\n",
    "    clear_session()\n",
    "    try:\n",
    "        del model\n",
    "    except NameError:\n",
    "        pass\n",
    "    collect()\n",
    "    \n",
    "    \n",
    "def import_dataset(filepath: str = \"./dataset.hdf5\") \\\n",
    "        -> Tuple[np.ndarray, np.ndarray, np.ndarray, \n",
    "                 np.ndarray, np.ndarray, np.ndarray]:\n",
    "    file = File(filepath, \"r\")\n",
    "    train_data = file.get(\"tr_data\")[()]\n",
    "    val_data = file.get(\"val_data\")[()]\n",
    "    test_data = file.get(\"ts_data\")[()]\n",
    "    train_labels = file.get(\"tr_labels\")[()]\n",
    "    val_labels = file.get(\"val_labels\")[()]\n",
    "    test_labels = file.get(\"ts_labels\")[()]\n",
    "    \n",
    "    return train_data, val_data, test_data, \\\n",
    "           train_labels, val_labels, test_labels\n",
    "\n",
    "\n",
    "def get_test_results(test_model: Model, test_data: np.ndarray, \n",
    "                     test_labels: np.ndarray) -> Tuple:\n",
    "    predicts = test_model.predict(test_data)\n",
    "    pred_out = np.argmax(predicts, axis=1)\n",
    "    test_out = np.argmax(test_labels, axis=1)\n",
    "    labels = [\"car\", \"heavy vehicles\", \"motorcycle\"]\n",
    "    \n",
    "    return accuracy_score(test_out, pred_out), \\\n",
    "           confusion_matrix(test_out, pred_out), \\\n",
    "           classification_report(test_out, pred_out, target_names=labels)\n",
    "\n",
    "\n",
    "def get_learn_rate(epoch: int) -> float:\n",
    "    if epoch <= 10:\n",
    "        lr = 1e-4\n",
    "    elif epoch <= 20:\n",
    "        lr = 5e-5\n",
    "    elif epoch <= 30:\n",
    "        lr = 1e-5\n",
    "    elif epoch <= 40:\n",
    "        lr = 5e-6\n",
    "    else:\n",
    "        lr = 1e-6\n",
    "    print(f\"Learning rate: {lr}\")\n",
    "    \n",
    "    return lr\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initialise Environment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Configure tensorflow to optimise GPU utilisation\n",
    "gpu_list = list_physical_devices(\"GPU\")\n",
    "for gpu in gpu_list:\n",
    "    set_memory_growth(gpu, True)\n",
    "del gpu_list\n",
    "\n",
    "# Fix tensorflow random seed\n",
    "set_random_seed(324)\n",
    "\n",
    "tr_dat, val_dat, ts_dat, tr_lbls, val_lbls, ts_lbls = import_dataset()\n",
    "\n",
    "in_shape = (tr_dat.shape[1], tr_dat.shape[2], tr_dat.shape[3])\n",
    "num_cls = tr_lbls.shape[1]\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(get_learn_rate)\n",
    "early_stopper = EarlyStopping(monitor=\"val_loss\", patience=10, verbose=1, \n",
    "                              mode=\"min\", restore_best_weights=True) \n",
    "\n",
    "# Test data is not needed in this notebook, so free up the memory\n",
    "del ts_dat\n",
    "del ts_lbls\n",
    "collect()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Creation and Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# window size 3x3\n",
    "refresh_session()\n",
    "data_gen = ImageDataGenerator(\n",
    "    rotation_range=45, width_shift_range=0.2, height_shift_range=0.2,\n",
    "    zoom_range=0.2, horizontal_flip=True)\n",
    "\n",
    "window_size = 3\n",
    "model = create_model(in_shape, num_cls, window_size=window_size)\n",
    "print(model.summary())\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"./trained_models/window_3x3_best.hdf5\", \n",
    "                             monitor=\"val_loss\", verbose=0, \n",
    "                             save_best_only=True, mode=\"min\")\n",
    "logger = CSVLogger(\"./training_logs/window_3x3_log.csv\")\n",
    "\n",
    "model.fit_generator(\n",
    "    data_gen.flow(tr_dat, tr_lbls, batch_size=32, shuffle=True), \n",
    "    steps_per_epoch=(len(tr_dat) / 32), epochs=50, verbose=2, \n",
    "    callbacks=[checkpoint, logger, lr_scheduler, early_stopper], \n",
    "    validation_data=(val_dat, val_lbls))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# window size 5x5\n",
    "refresh_session()\n",
    "data_gen = ImageDataGenerator(\n",
    "    rotation_range=45, width_shift_range=0.2, height_shift_range=0.2,\n",
    "    zoom_range=0.2, horizontal_flip=True)\n",
    "\n",
    "window_size = 5\n",
    "model = create_model(in_shape, num_cls, window_size=window_size)\n",
    "print(model.summary())\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"./trained_models/window_5x5_best.hdf5\", \n",
    "                             monitor=\"val_loss\", verbose=0, \n",
    "                             save_best_only=True, mode=\"min\")\n",
    "logger = CSVLogger(\"./training_logs/window_5x5_log.csv\")\n",
    "\n",
    "model.fit_generator(\n",
    "    data_gen.flow(tr_dat, tr_lbls, batch_size=32, shuffle=True), \n",
    "    steps_per_epoch=(len(tr_dat) / 32), epochs=50, verbose=2, \n",
    "    callbacks=[checkpoint, logger, lr_scheduler, early_stopper], \n",
    "    validation_data=(val_dat, val_lbls))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training phase is complete: free training data memory\n",
    "del tr_dat\n",
    "del tr_lbls\n",
    "refresh_session()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Visualisation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "acc_scores = dict()\n",
    "conf_matrices = dict()\n",
    "class_reports = dict()\n",
    "\n",
    "plt.style.use(\"ggplot\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2x2 baseline\n",
    "refresh_session()\n",
    "model = load_model(\"./trained_models/vgg13_best.hdf5\")\n",
    "acc_scores[2], conf_matrices[2], class_reports[2] \\\n",
    "    = get_test_results(model, val_dat, val_lbls)\n",
    "\n",
    "print(f\"Validation accuracy for 2x2 window size: {acc_scores[2]}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrices[2])\n",
    "print(class_reports[2])\n",
    "\n",
    "log_2x2 = read_csv(\"./training_logs/vgg13_log.csv\")\n",
    "\n",
    "plt.figure(figsize=[12.5, 12.5])\n",
    "plt.subplot(211)\n",
    "plt.plot(log_2x2[\"val_loss\"], label=\"loss\")\n",
    "plt.plot(log_2x2[\"val_acc\"], label=\"accuracy\")\n",
    "plt.title(\"Validation\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(log_2x2[\"loss\"], label=\"train loss\")\n",
    "plt.plot(log_2x2[\"val_loss\"], label=\"validation loss\")\n",
    "plt.title(\"Loss\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(log_2x2[\"acc\"], label=\"train accuracy\")\n",
    "plt.plot(log_2x2[\"val_acc\"], label=\"validation accuracy\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3x3\n",
    "refresh_session()\n",
    "model = load_model(\"./trained_models/window_3x3_best.hdf5\")\n",
    "acc_scores[3], conf_matrices[3], class_reports[3] \\\n",
    "    = get_test_results(model, val_dat, val_lbls)\n",
    "\n",
    "print(f\"Validation accuracy for 3x3 window size: {acc_scores[3]}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrices[3])\n",
    "print(class_reports[3])\n",
    "\n",
    "log_3x3 = read_csv(\"./training_logs/window_3x3_log.csv\")\n",
    "\n",
    "plt.figure(figsize=[12.5, 12.5])\n",
    "plt.subplot(311)\n",
    "plt.plot(log_3x3[\"val_loss\"], label=\"loss\")\n",
    "plt.plot(log_3x3[\"val_acc\"], label=\"accuracy\")\n",
    "plt.title(\"Validation\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.plot(log_3x3[\"loss\"], label=\"train loss\")\n",
    "plt.plot(log_3x3[\"val_loss\"], label=\"validation loss\")\n",
    "plt.title(\"Loss\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.plot(log_3x3[\"acc\"], label=\"train accuracy\")\n",
    "plt.plot(log_3x3[\"val_acc\"], label=\"validation accuracy\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n",
    "\n",
    "\n",
    "# 5x5\n",
    "refresh_session()\n",
    "model = load_model(\"./trained_models/window_5x5_best.hdf5\")\n",
    "acc_scores[5], conf_matrices[5], class_reports[5] \\\n",
    "    = get_test_results(model, val_dat, val_lbls)\n",
    "\n",
    "print(f\"Validation accuracy for 5x5 window size: {acc_scores[5]}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrices[5])\n",
    "print(class_reports[5])\n",
    "\n",
    "log_5x5 = read_csv(\"./training_logs/window_5x5_log.csv\")\n",
    "\n",
    "plt.figure(figsize=[12.5, 12.5])\n",
    "plt.subplot(311)\n",
    "plt.plot(log_5x5[\"val_loss\"], label=\"loss\")\n",
    "plt.plot(log_5x5[\"val_acc\"], label=\"accuracy\")\n",
    "plt.title(\"Validation\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.plot(log_5x5[\"loss\"], label=\"train loss\")\n",
    "plt.plot(log_5x5[\"val_loss\"], label=\"validation loss\")\n",
    "plt.title(\"Loss\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.plot(log_5x5[\"acc\"], label=\"train accuracy\")\n",
    "plt.plot(log_5x5[\"val_acc\"], label=\"validation accuracy\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Cross-Model Comparison"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10, 7.5])\n",
    "plt.subplot(211)\n",
    "plt.plot(log_2x2[\"val_loss\"], label=\"kernel 2x2\")\n",
    "plt.plot(log_3x3[\"val_loss\"], label=\"kernel 3x3\")\n",
    "plt.plot(log_5x5[\"val_loss\"], label=\"kernel 5x5\")\n",
    "plt.title(\"Validation Loss\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(log_2x2[\"val_acc\"], label=\"kernel 2x2\")\n",
    "plt.plot(log_3x3[\"val_acc\"], label=\"kernel 3x3\")\n",
    "plt.plot(log_5x5[\"val_acc\"], label=\"kernel 5x5\")\n",
    "plt.title(\"Validation Accuracy\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Final Model Test\n",
    "\n",
    "After hyper-parameter tuning and regularisation, we have derived the\n",
    "final model defined below. In this notebook, we run it against the final\n",
    "test data.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gc import collect\n",
    "from typing import Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from h5py import File\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import accuracy_score, classification_report, \\\n",
    "    confusion_matrix\n",
    "from tensorflow.config.experimental import list_physical_devices, \\\n",
    "    set_memory_growth\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.callbacks import CSVLogger, LearningRateScheduler, \\\n",
    "    ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Input, MaxPool2D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.random import set_random_seed\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Creation Function\n",
    "\n",
    "# TODO: RUN THE OTHER SCRIPTS AND DETERMINE THE BEST MODEL CONFIGURATION THEN INPUT BELOW"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Final model\n",
    "def create_model(input_shape: Tuple[int, int, int], num_classes: int) -> Model:\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    layer = Conv2D(filters=64, kernel_size=(3, 3), strides=1, padding=\"same\", \n",
    "                   activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                   bias_initializer=\"he_normal\")(inputs)\n",
    "    layer = Conv2D(filters=64, kernel_size=(3, 3), strides=1, padding=\"same\", \n",
    "                   activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = MaxPool2D(pool_size=(2, 2), strides=2)(layer)\n",
    "\n",
    "    layer = Conv2D(filters=128, kernel_size=(3, 3), strides=1, padding=\"same\", \n",
    "                   activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = Conv2D(filters=128, kernel_size=(3, 3), strides=1, padding=\"same\", \n",
    "                   activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = MaxPool2D(pool_size=(2, 2), strides=2)(layer)\n",
    "    \n",
    "    layer = Conv2D(filters=256, kernel_size=(3, 3), strides=1, padding=\"same\", \n",
    "                   activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = Conv2D(filters=256, kernel_size=(3, 3), strides=1, padding=\"same\", \n",
    "                   activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = MaxPool2D(pool_size=(2, 2), strides=2)(layer)\n",
    "    \n",
    "    layer = Conv2D(filters=512, kernel_size=(3, 3), strides=1, padding=\"same\", \n",
    "                   activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = Conv2D(filters=512, kernel_size=(3, 3), strides=1, padding=\"same\", \n",
    "                   activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = MaxPool2D(pool_size=(2, 2), strides=2)(layer)\n",
    "    \n",
    "    layer = Conv2D(filters=512, kernel_size=(3, 3), strides=1, padding=\"same\", \n",
    "                   activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = Conv2D(filters=512, kernel_size=(3, 3), strides=1, padding=\"same\", \n",
    "                   activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = MaxPool2D(pool_size=(2, 2), strides=2)(layer)\n",
    "    \n",
    "    layer = Flatten()(layer)\n",
    "    layer = Dense(units=4096, activation=\"relu\", \n",
    "                  kernel_initializer=\"he_normal\",\n",
    "                  bias_initializer=\"he_normal\")(layer)\n",
    "    layer = Dense(units=4096, activation=\"relu\", \n",
    "                  kernel_initializer=\"he_normal\",\n",
    "                  bias_initializer=\"he_normal\")(layer)\n",
    "    layer = Dense(num_classes, activation=\"softmax\")(layer)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=layer)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), \n",
    "                  loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Other Functions\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def refresh_session():\n",
    "    # Call this before training a new model, to free up memory from the \n",
    "    # previous model\n",
    "    clear_session()\n",
    "    try:\n",
    "        del model\n",
    "    except NameError:\n",
    "        pass\n",
    "    collect()\n",
    "    \n",
    "    \n",
    "def import_dataset(filepath: str = \"./dataset.hdf5\") \\\n",
    "        -> Tuple[np.ndarray, np.ndarray, np.ndarray, \n",
    "                 np.ndarray, np.ndarray, np.ndarray]:\n",
    "    file = File(filepath, \"r\")\n",
    "    train_data = file.get(\"tr_data\")[()]\n",
    "    val_data = file.get(\"val_data\")[()]\n",
    "    test_data = file.get(\"ts_data\")[()]\n",
    "    train_labels = file.get(\"tr_labels\")[()]\n",
    "    val_labels = file.get(\"val_labels\")[()]\n",
    "    test_labels = file.get(\"ts_labels\")[()]\n",
    "    \n",
    "    return train_data, val_data, test_data, \\\n",
    "           train_labels, val_labels, test_labels\n",
    "\n",
    "\n",
    "def get_test_results(test_model: Model, test_data: np.ndarray, \n",
    "                     test_labels: np.ndarray) -> Tuple:\n",
    "    predicts = test_model.predict(test_data)\n",
    "    pred_out = np.argmax(predicts, axis=1)\n",
    "    test_out = np.argmax(test_labels, axis=1)\n",
    "    labels = [\"car\", \"heavy vehicles\", \"motorcycle\"]\n",
    "    \n",
    "    return accuracy_score(test_out, pred_out), \\\n",
    "           confusion_matrix(test_out, pred_out), \\\n",
    "           classification_report(test_out, pred_out, target_names=labels)\n",
    "\n",
    "\n",
    "def get_learn_rate(epoch: int) -> float:\n",
    "    if epoch <= 10:\n",
    "        lr = 1e-4\n",
    "    elif epoch <= 20:\n",
    "        lr = 5e-5\n",
    "    elif epoch <= 30:\n",
    "        lr = 1e-5\n",
    "    elif epoch <= 40:\n",
    "        lr = 5e-6\n",
    "    else:\n",
    "        lr = 1e-6\n",
    "    print(f\"Learning rate: {lr}\")\n",
    "    \n",
    "    return lr\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initialise Environment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Configure tensorflow to optimise GPU utilisation\n",
    "gpu_list = list_physical_devices(\"GPU\")\n",
    "for gpu in gpu_list:\n",
    "    set_memory_growth(gpu, True)\n",
    "del gpu_list\n",
    "\n",
    "# Fix tensorflow random seed\n",
    "set_random_seed(324)\n",
    "\n",
    "tr_dat, val_dat, ts_dat, tr_lbls, val_lbls, ts_lbls = import_dataset()\n",
    "\n",
    "in_shape = (tr_dat.shape[1], tr_dat.shape[2], tr_dat.shape[3])\n",
    "num_cls = tr_lbls.shape[1]\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(get_learn_rate)\n",
    "early_stopper = EarlyStopping(monitor=\"val_loss\", patience=10, verbose=1, \n",
    "                              mode=\"min\", restore_best_weights=True) \n",
    "\n",
    "# Combine training and validation data into one set for the final training\n",
    "final_tr_dat = np.concatenate((tr_dat, val_dat), axis=0)\n",
    "final_tr_lbls = np.concatenate((tr_lbls, val_lbls), axis=0)\n",
    "\n",
    "# tr and val variables no longer needed, delete to free up space\n",
    "del tr_dat\n",
    "del val_dat\n",
    "del tr_lbls\n",
    "del val_lbls\n",
    "collect()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Creation and Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "refresh_session()\n",
    "data_gen = ImageDataGenerator(\n",
    "    rotation_range=45, width_shift_range=0.2, height_shift_range=0.2,\n",
    "    zoom_range=0.2, horizontal_flip=True)\n",
    "\n",
    "model = create_model(in_shape, num_cls)\n",
    "print(model.summary())\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"./trained_models/grandfinal_best.hdf5\", \n",
    "                             monitor=\"val_loss\", verbose=0, \n",
    "                             save_best_only=True, mode=\"min\")\n",
    "logger = CSVLogger(\"./training_logs/grandfinal_log.csv\")\n",
    "\n",
    "model.fit_generator(\n",
    "    data_gen.flow(final_tr_dat, final_tr_lbls, batch_size=32, shuffle=True),\n",
    "    steps_per_epoch=(len(tr_dat) / 32), epochs=100, verbose=2, \n",
    "    callbacks=[checkpoint, logger, lr_scheduler, early_stopper], \n",
    "    validation_data=(ts_dat, ts_lbls))\n",
    "\n",
    "\n",
    "# Training phase is complete: free training data memory\n",
    "del final_tr_dat\n",
    "del final_tr_lbls\n",
    "refresh_session()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Visualisation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "refresh_session()\n",
    "model = load_model(\"./trained_models/grandfinal_best.hdf5\")\n",
    "acc_score, conf_matrix, class_report = \\\n",
    "    get_test_results(model, ts_dat, ts_lbls)\n",
    "\n",
    "print(f\"Test accuracy: {acc_score}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(class_report)\n",
    "\n",
    "log = read_csv(\"./training_logs/grandfinal_log.csv\")\n",
    "\n",
    "plt.figure(figsize=[12.5, 12.5])\n",
    "plt.subplot(311)\n",
    "plt.plot(log[\"val_loss\"], label=\"loss\")\n",
    "plt.plot(log[\"val_acc\"], label=\"accuracy\")\n",
    "plt.title(\"Test\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.plot(log[\"loss\"], label=\"train loss\")\n",
    "plt.plot(log[\"val_loss\"], label=\"validation loss\")\n",
    "plt.title(\"Loss\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.plot(log[\"acc\"], label=\"train accuracy\")\n",
    "plt.plot(log[\"val_acc\"], label=\"validation accuracy\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
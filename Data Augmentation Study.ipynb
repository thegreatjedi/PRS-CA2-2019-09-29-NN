{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Augmentation Study\n",
    "\n",
    "This notebook studies the impact of data augmentation on our training \n",
    "process. In this study, we will use a VGG16-based architecture for\n",
    "comparison.\n",
    "\n",
    "Reference:\n",
    " - https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    " - https://towardsdatascience.com/cnn-architectures-a-deep-dive-a99441d18049"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from typing import Tuple\n",
    "from gc import collect\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from h5py import File\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, \\\n",
    "    classification_report\n",
    "from tensorflow import ConfigProto, Session\n",
    "from tensorflow.keras.backend import set_session, clear_session\n",
    "from tensorflow.keras.callbacks import CSVLogger, LearningRateScheduler, \\\n",
    "    ModelCheckpoint\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Input, MaxPool2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.random import set_random_seed\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Improve GPU memory utilisation\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.log_device_placement = True\n",
    "sess = Session(config=config)\n",
    "set_session(sess)\n",
    "\n",
    "# Fix tensorflow random seed\n",
    "set_random_seed(324)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Creation Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# VGG-16\n",
    "def create_vgg16(input_shape: Tuple[int, int, int], num_classes: int) -> Model:\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    layer = Conv2D(filters=64, kernel_size=(3, 3), strides=1, padding=\"same\", \n",
    "                   activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                   bias_initializer=\"he_normal\")(inputs)\n",
    "    layer = Conv2D(filters=64, kernel_size=(3, 3), strides=1, padding=\"same\", \n",
    "                   activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = MaxPool2D(pool_size=(2, 2), strides=2)(layer)\n",
    "\n",
    "    layer = Conv2D(filters=128, kernel_size=(3, 3), strides=1, padding=\"same\", \n",
    "                   activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = Conv2D(filters=128, kernel_size=(3, 3), strides=1, padding=\"same\", \n",
    "                   activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = MaxPool2D(pool_size=(2, 2), strides=2)(layer)\n",
    "    \n",
    "    layer = Conv2D(filters=256, kernel_size=(3, 3), strides=1, padding=\"same\", \n",
    "                   activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = Conv2D(filters=256, kernel_size=(3, 3), strides=1, padding=\"same\", \n",
    "                   activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = Conv2D(filters=256, kernel_size=(3, 3), strides=1, padding=\"same\", \n",
    "                   activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = MaxPool2D(pool_size=(2, 2), strides=2)(layer)\n",
    "    \n",
    "    layer = Conv2D(filters=512, kernel_size=(3, 3), strides=1, padding=\"same\", \n",
    "                   activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = Conv2D(filters=512, kernel_size=(3, 3), strides=1, padding=\"same\", \n",
    "                   activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = Conv2D(filters=512, kernel_size=(3, 3), strides=1, padding=\"same\", \n",
    "                   activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = MaxPool2D(pool_size=(2, 2), strides=2)(layer)\n",
    "    \n",
    "    layer = Conv2D(filters=512, kernel_size=(3, 3), strides=1, padding=\"same\", \n",
    "                   activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = Conv2D(filters=512, kernel_size=(3, 3), strides=1, padding=\"same\", \n",
    "                   activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = Conv2D(filters=512, kernel_size=(3, 3), strides=1, padding=\"same\", \n",
    "                   activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                   bias_initializer=\"he_normal\")(layer)\n",
    "    layer = MaxPool2D(pool_size=(2, 2), strides=2)(layer)\n",
    "    \n",
    "    layer = Flatten()(layer)\n",
    "    layer = Dense(units=4096, activation=\"relu\", \n",
    "                  kernel_initializer=\"he_normal\",\n",
    "                  bias_initializer=\"he_normal\")(layer)\n",
    "    layer = Dense(units=4096, activation=\"relu\", \n",
    "                  kernel_initializer=\"he_normal\",\n",
    "                  bias_initializer=\"he_normal\")(layer)\n",
    "    layer = Dense(num_classes, activation=\"softmax\")(layer)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=layer)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), \n",
    "                  loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Other Functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def import_dataset(filepath: str = \"./dataset.hdf5\") \\\n",
    "        -> Tuple[np.ndarray, np.ndarray, np.ndarray, \n",
    "                 np.ndarray, np.ndarray, np.ndarray]:\n",
    "    file = File(filepath, \"r\")\n",
    "    train_data = file.get(\"tr_data\")[()]\n",
    "    val_data = file.get(\"val_data\")[()]\n",
    "    test_data = file.get(\"ts_data\")[()]\n",
    "    train_labels = file.get(\"tr_labels\")[()]\n",
    "    val_labels = file.get(\"val_labels\")[()]\n",
    "    test_labels = file.get(\"ts_labels\")[()]\n",
    "    \n",
    "    return train_data, val_data, test_data, \\\n",
    "           train_labels, val_labels, test_labels\n",
    "\n",
    "\n",
    "def get_test_results(test_model: Model, test_data: np.ndarray, \n",
    "                     test_labels: np.ndarray) -> Tuple:\n",
    "    predicts = test_model.predict(test_data)\n",
    "    pred_out = np.argmax(predicts, axis=1)\n",
    "    test_out = np.argmax(test_labels, axis=1)\n",
    "    labels = [\"car\", \"heavy vehicles\", \"motorcycle\"]\n",
    "    \n",
    "    return accuracy_score(test_out, pred_out), \\\n",
    "           confusion_matrix(test_out, pred_out), \\\n",
    "           classification_report(test_out, pred_out, target_names=labels)\n",
    "\n",
    "\n",
    "def get_learn_rate(epoch: int) -> float:\n",
    "    lr = 1e-4\n",
    "    if epoch > 10:\n",
    "        lr = 1e-5\n",
    "    elif epoch > 20:\n",
    "        lr = 1e-6\n",
    "    elif epoch > 30:\n",
    "        lr = 1e-7\n",
    "    elif epoch > 40:\n",
    "        lr = 1e-8\n",
    "    elif epoch > 50:\n",
    "        lr = 1e-9\n",
    "        \n",
    "    print(f\"Learning rate: {lr}\")\n",
    "    \n",
    "    return lr\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Initialisation\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "tr_dat, val_dat, ts_dat, tr_lbls, val_lbls, ts_lbls = import_dataset()\n",
    "\n",
    "in_shape = (tr_dat.shape[1], tr_dat.shape[2], tr_dat.shape[3])\n",
    "num_cls = tr_lbls.shape[1]\n",
    "\n",
    "model = None\n",
    "lr_scheduler = LearningRateScheduler(get_learn_rate)\n",
    "\n",
    "# Clear test data from memory as we're not using it here\n",
    "del ts_dat\n",
    "del ts_lbls\n",
    "collect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Creation & Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Nelson\\.conda\\envs\\prmls_ca2\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 256, 256, 3)]     0         \n_________________________________________________________________\nconv2d (Conv2D)              (None, 256, 256, 64)      1792      \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 256, 256, 64)      36928     \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 128, 128, 64)      0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 128, 128, 128)     73856     \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 128, 128, 128)     147584    \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 64, 64, 128)       0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 64, 64, 256)       295168    \n_________________________________________________________________\nconv2d_5 (Conv2D)            (None, 64, 64, 256)       590080    \n_________________________________________________________________\nconv2d_6 (Conv2D)            (None, 64, 64, 256)       590080    \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 32, 32, 256)       0         \n_________________________________________________________________\nconv2d_7 (Conv2D)            (None, 32, 32, 512)       1180160   \n_________________________________________________________________\nconv2d_8 (Conv2D)            (None, 32, 32, 512)       2359808   \n_________________________________________________________________\nconv2d_9 (Conv2D)            (None, 32, 32, 512)       2359808   \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 16, 16, 512)       0         \n_________________________________________________________________\nconv2d_10 (Conv2D)           (None, 16, 16, 512)       2359808   \n_________________________________________________________________\nconv2d_11 (Conv2D)           (None, 16, 16, 512)       2359808   \n_________________________________________________________________\nconv2d_12 (Conv2D)           (None, 16, 16, 512)       2359808   \n_________________________________________________________________\nmax_pooling2d_4 (MaxPooling2 (None, 8, 8, 512)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 32768)             0         \n_________________________________________________________________\ndense (Dense)                (None, 4096)              134221824 \n_________________________________________________________________\ndense_1 (Dense)              (None, 4096)              16781312  \n_________________________________________________________________\ndense_2 (Dense)              (None, 3)                 12291     \n=================================================================\nTotal params: 165,730,115\nTrainable params: 165,730,115\nNon-trainable params: 0\n_________________________________________________________________\nNone\nTrain on 2044 samples, validate on 512 samples\n",
      "Epoch 1/25\n",
      "2044/2044 - 37s - loss: 2.2726 - acc: 0.3523 - val_loss: 1.0971 - val_acc: 0.3379\n",
      "Epoch 2/25\n",
      "2044/2044 - 24s - loss: 1.0987 - acc: 0.3513 - val_loss: 1.0981 - val_acc: 0.3535\n",
      "Epoch 3/25\n",
      "2044/2044 - 24s - loss: 1.0946 - acc: 0.3743 - val_loss: 1.1141 - val_acc: 0.3242\n",
      "Epoch 4/25\n",
      "2044/2044 - 31s - loss: 1.0891 - acc: 0.3933 - val_loss: 1.0727 - val_acc: 0.4453\n",
      "Epoch 5/25\n",
      "2044/2044 - 30s - loss: 1.0758 - acc: 0.3992 - val_loss: 1.0682 - val_acc: 0.4180\n",
      "Epoch 6/25\n",
      "2044/2044 - 30s - loss: 1.0522 - acc: 0.4442 - val_loss: 1.0364 - val_acc: 0.4688\n",
      "Epoch 7/25\n",
      "2044/2044 - 30s - loss: 0.9975 - acc: 0.5039 - val_loss: 1.0123 - val_acc: 0.4746\n",
      "Epoch 8/25\n",
      "2044/2044 - 30s - loss: 0.9493 - acc: 0.5342 - val_loss: 0.9720 - val_acc: 0.5332\n",
      "Epoch 9/25\n",
      "2044/2044 - 31s - loss: 0.9147 - acc: 0.5406 - val_loss: 0.9494 - val_acc: 0.5586\n",
      "Epoch 10/25\n",
      "2044/2044 - 23s - loss: 0.8236 - acc: 0.6057 - val_loss: 1.0136 - val_acc: 0.5703\n",
      "Epoch 11/25\n",
      "2044/2044 - 31s - loss: 0.7345 - acc: 0.6614 - val_loss: 0.9292 - val_acc: 0.5488\n",
      "Epoch 12/25\n",
      "2044/2044 - 23s - loss: 0.6554 - acc: 0.7128 - val_loss: 0.9809 - val_acc: 0.5332\n",
      "Epoch 13/25\n",
      "2044/2044 - 23s - loss: 0.5282 - acc: 0.7637 - val_loss: 1.1547 - val_acc: 0.5605\n",
      "Epoch 14/25\n",
      "2044/2044 - 24s - loss: 0.4008 - acc: 0.8307 - val_loss: 1.3224 - val_acc: 0.5723\n",
      "Epoch 15/25\n",
      "2044/2044 - 24s - loss: 0.2723 - acc: 0.8885 - val_loss: 1.4135 - val_acc: 0.5664\n",
      "Epoch 16/25\n",
      "2044/2044 - 24s - loss: 0.2360 - acc: 0.9159 - val_loss: 2.0150 - val_acc: 0.5566\n",
      "Epoch 17/25\n",
      "2044/2044 - 24s - loss: 0.2024 - acc: 0.9276 - val_loss: 1.6004 - val_acc: 0.5176\n",
      "Epoch 18/25\n",
      "2044/2044 - 25s - loss: 0.1368 - acc: 0.9521 - val_loss: 2.1779 - val_acc: 0.5488\n",
      "Epoch 19/25\n",
      "2044/2044 - 24s - loss: 0.1068 - acc: 0.9643 - val_loss: 2.0760 - val_acc: 0.5645\n",
      "Epoch 20/25\n",
      "2044/2044 - 24s - loss: 0.0607 - acc: 0.9829 - val_loss: 3.1362 - val_acc: 0.5430\n",
      "Epoch 21/25\n",
      "2044/2044 - 24s - loss: 0.0818 - acc: 0.9746 - val_loss: 2.2076 - val_acc: 0.5430\n",
      "Epoch 22/25\n",
      "2044/2044 - 24s - loss: 0.0564 - acc: 0.9834 - val_loss: 2.1318 - val_acc: 0.5781\n",
      "Epoch 23/25\n",
      "2044/2044 - 25s - loss: 0.0259 - acc: 0.9927 - val_loss: 2.5132 - val_acc: 0.5469\n",
      "Epoch 24/25\n",
      "2044/2044 - 24s - loss: 0.0226 - acc: 0.9946 - val_loss: 2.3239 - val_acc: 0.5547\n",
      "Epoch 25/25\n",
      "2044/2044 - 23s - loss: 0.0220 - acc: 0.9927 - val_loss: 3.3051 - val_acc: 0.5508\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# No data augmentation\n",
    "model = create_vgg16(in_shape, num_cls)\n",
    "print(model.summary())\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"./trained_models/no_augmentation_best.hdf5\", \n",
    "                             monitor=\"val_loss\", verbose=0, \n",
    "                             save_best_only=True, mode=\"min\")\n",
    "logger = CSVLogger(\"./training_logs/no_augmentation_log.csv\")\n",
    "\n",
    "model.fit(tr_dat, tr_lbls, batch_size=32, validation_data=(val_dat, val_lbls), \n",
    "          epochs=20, verbose=2, shuffle=True, \n",
    "          callbacks=[checkpoint, logger, lr_scheduler])\n",
    "\n",
    "model.save(f\"./trained_models/no_augmentation_20epoch.hdf5\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Nelson\\.conda\\envs\\prmls_ca2\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 256, 256, 3)]     0         \n_________________________________________________________________\nconv2d (Conv2D)              (None, 256, 256, 64)      1792      \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 256, 256, 64)      36928     \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 128, 128, 64)      0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 128, 128, 128)     73856     \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 128, 128, 128)     147584    \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 64, 64, 128)       0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 64, 64, 256)       295168    \n_________________________________________________________________\nconv2d_5 (Conv2D)            (None, 64, 64, 256)       590080    \n_________________________________________________________________\nconv2d_6 (Conv2D)            (None, 64, 64, 256)       590080    \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 32, 32, 256)       0         \n_________________________________________________________________\nconv2d_7 (Conv2D)            (None, 32, 32, 512)       1180160   \n_________________________________________________________________\nconv2d_8 (Conv2D)            (None, 32, 32, 512)       2359808   \n_________________________________________________________________\nconv2d_9 (Conv2D)            (None, 32, 32, 512)       2359808   \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 16, 16, 512)       0         \n_________________________________________________________________\nconv2d_10 (Conv2D)           (None, 16, 16, 512)       2359808   \n_________________________________________________________________\nconv2d_11 (Conv2D)           (None, 16, 16, 512)       2359808   \n_________________________________________________________________\nconv2d_12 (Conv2D)           (None, 16, 16, 512)       2359808   \n_________________________________________________________________\nmax_pooling2d_4 (MaxPooling2 (None, 8, 8, 512)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 32768)             0         \n_________________________________________________________________\ndense (Dense)                (None, 4096)              134221824 \n_________________________________________________________________\ndense_1 (Dense)              (None, 4096)              16781312  \n_________________________________________________________________\ndense_2 (Dense)              (None, 3)                 12291     \n=================================================================\nTotal params: 165,730,115\nTrainable params: 165,730,115\nNon-trainable params: 0\n_________________________________________________________________\nNone\n",
      "Learning rate changed to 0.001\nEpoch 1/25\n",
      "128/127 - 41s - loss: 95.8426 - acc: 0.3356 - val_loss: 1.0994 - val_acc: 0.3379\n",
      "Learning rate changed to 0.001\nEpoch 2/25\n",
      "128/127 - 25s - loss: 1.1091 - acc: 0.3332 - val_loss: 1.1301 - val_acc: 0.3379\n",
      "Learning rate changed to 0.001\nEpoch 3/25\n",
      "128/127 - 30s - loss: 1.1017 - acc: 0.3224 - val_loss: 1.0986 - val_acc: 0.3379\n",
      "Learning rate changed to 0.001\nEpoch 4/25\n",
      "128/127 - 30s - loss: 1.1002 - acc: 0.3209 - val_loss: 1.0984 - val_acc: 0.3379\n",
      "Learning rate changed to 0.001\nEpoch 5/25\n",
      "128/127 - 24s - loss: 1.1099 - acc: 0.3434 - val_loss: 1.0988 - val_acc: 0.3242\n",
      "Learning rate changed to 0.001\nEpoch 6/25\n",
      "128/127 - 24s - loss: 1.1047 - acc: 0.3244 - val_loss: 1.0985 - val_acc: 0.3379\n",
      "Learning rate changed to 0.0005\nEpoch 7/25\n",
      "128/127 - 24s - loss: 1.0993 - acc: 0.3351 - val_loss: 1.0986 - val_acc: 0.3379\n",
      "Learning rate changed to 0.0005\nEpoch 8/25\n",
      "128/127 - 24s - loss: 1.0990 - acc: 0.3317 - val_loss: 1.0985 - val_acc: 0.3379\n",
      "Learning rate changed to 0.0005\nEpoch 9/25\n",
      "128/127 - 24s - loss: 1.0988 - acc: 0.3273 - val_loss: 1.0984 - val_acc: 0.3379\n",
      "Learning rate changed to 0.0005\nEpoch 10/25\n",
      "128/127 - 24s - loss: 1.0988 - acc: 0.3322 - val_loss: 1.0984 - val_acc: 0.3379\n",
      "Learning rate changed to 0.0005\nEpoch 11/25\n",
      "128/127 - 25s - loss: 1.0989 - acc: 0.3263 - val_loss: 1.0984 - val_acc: 0.3379\n",
      "Learning rate changed to 0.0005\nEpoch 12/25\n",
      "128/127 - 32s - loss: 1.0989 - acc: 0.3278 - val_loss: 1.0984 - val_acc: 0.3379\n",
      "Learning rate changed to 0.0005\nEpoch 13/25\n",
      "128/127 - 25s - loss: 1.0988 - acc: 0.3361 - val_loss: 1.0985 - val_acc: 0.3359\n",
      "Learning rate changed to 0.0005\nEpoch 14/25\n",
      "128/127 - 25s - loss: 1.1430 - acc: 0.3400 - val_loss: 1.1063 - val_acc: 0.3125\n",
      "Learning rate changed to 0.0005\nEpoch 15/25\n",
      "128/127 - 24s - loss: 1.2346 - acc: 0.3361 - val_loss: 1.0989 - val_acc: 0.3242\n",
      "Learning rate changed to 0.0005\nEpoch 16/25\n",
      "128/127 - 30s - loss: 1.0991 - acc: 0.3337 - val_loss: 1.0981 - val_acc: 0.3438\n",
      "Learning rate changed to 0.0005\nEpoch 17/25\n",
      "128/127 - 30s - loss: 1.0963 - acc: 0.3645 - val_loss: 1.0941 - val_acc: 0.3770\n",
      "Learning rate changed to 0.0005\nEpoch 18/25\n",
      "128/127 - 26s - loss: 1.0975 - acc: 0.3532 - val_loss: 1.0975 - val_acc: 0.3438\n",
      "Learning rate changed to 0.0005\nEpoch 19/25\n",
      "128/127 - 26s - loss: 1.0976 - acc: 0.3381 - val_loss: 1.0984 - val_acc: 0.3770\n",
      "Learning rate changed to 0.0005\nEpoch 20/25\n",
      "128/127 - 24s - loss: 1.0994 - acc: 0.3263 - val_loss: 1.0982 - val_acc: 0.3457\n",
      "Learning rate changed to 0.0005\nEpoch 21/25\n",
      "128/127 - 25s - loss: 1.0986 - acc: 0.3381 - val_loss: 1.0956 - val_acc: 0.3652\n",
      "Learning rate changed to 0.0005\nEpoch 22/25\n",
      "128/127 - 25s - loss: 1.0971 - acc: 0.3704 - val_loss: 1.1017 - val_acc: 0.3223\n",
      "Learning rate changed to 0.0005\nEpoch 23/25\n",
      "128/127 - 31s - loss: 1.0991 - acc: 0.3434 - val_loss: 1.0919 - val_acc: 0.3594\n",
      "Learning rate changed to 0.0005\nEpoch 24/25\n",
      "128/127 - 25s - loss: 1.0947 - acc: 0.3581 - val_loss: 1.0937 - val_acc: 0.3789\n",
      "Learning rate changed to 0.0005\nEpoch 25/25\n",
      "128/127 - 25s - loss: 1.0926 - acc: 0.3787 - val_loss: 1.0938 - val_acc: 0.3887\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Use data augmentation\n",
    "data_gen = ImageDataGenerator(\n",
    "    rotation_range=45, width_shift_range=0.2, height_shift_range=0.2,\n",
    "    zoom_range=0.2, horizontal_flip=True)\n",
    "\n",
    "model = create_vgg16(in_shape, num_cls)\n",
    "print(model.summary())\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"./trained_models/augmented_best.hdf5\", \n",
    "                             monitor=\"val_loss\", verbose=0, \n",
    "                             save_best_only=True, mode=\"min\")\n",
    "logger = CSVLogger(\"./training_logs/augmented_log.csv\")\n",
    "\n",
    "model.fit_generator(\n",
    "    data_gen.flow(tr_dat, tr_lbls, batch_size=32, shuffle=True), \n",
    "    steps_per_epoch=(len(tr_dat) / 32), epochs=20, verbose=2, \n",
    "    callbacks=[checkpoint, logger, lr_scheduler], \n",
    "    validation_data=(val_dat, val_lbls))\n",
    "\n",
    "model.save(f\"./trained_models/augmented_20epoch.hdf5\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training stage complete: free training data memory\n",
    "del tr_dat\n",
    "del tr_lbls\n",
    "collect()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Visualisation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "acc_scores = dict()\n",
    "conf_matrices = dict()\n",
    "class_reports = dict()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Before Data Augmentation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_unaugmented = create_vgg16(in_shape, num_cls)\n",
    "model_unaugmented.load_weights(\"./trained_models/no_augmentation_best.hdf5\")\n",
    "model_unaugmented.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "                          loss=\"categorical_crossentropy\", \n",
    "                          metrics=[\"accuracy\"])\n",
    "acc_scores[\"unaugmented\"], conf_matrices[\"unaugmented\"], \\\n",
    "class_reports[\"unaugmented\"] \\\n",
    "    = get_test_results(model_unaugmented, val_dat, val_lbls)\n",
    "\n",
    "print(f\"Validation accuracy before augmentation: {acc_scores['unaugmented']}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrices['unaugmented'])\n",
    "print(class_reports['unaugmented'])\n",
    "\n",
    "unaugmented_log = read_csv(\"./training_logs/no_augmentation_log.csv\")\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=[10, 7.5])\n",
    "plt.subplot(211)\n",
    "plt.xticks([0, 5, 10, 15, 20])\n",
    "plt.plot(unaugmented_log[\"loss\"], label=\"train loss\")\n",
    "plt.plot(unaugmented_log[\"val_loss\"], label=\"validation loss\")\n",
    "plt.title(\"Loss\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.xticks([0, 5, 10, 15, 20])\n",
    "plt.plot(unaugmented_log[\"acc\"], label=\"train accuracy\")\n",
    "plt.plot(unaugmented_log[\"val_acc\"], label=\"validation accuracy\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n",
    "\n",
    "# Free memory\n",
    "clear_session()\n",
    "collect()\n",
    "del model_unaugmented\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### After Data Augmentation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_augmented = create_vgg16(in_shape, num_cls)\n",
    "model_augmented.load_weights(\"./trained_models/augmented_best.hdf5\")\n",
    "model_augmented.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "                        loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "acc_scores[\"augmented\"], conf_matrices[\"augmented\"], \\\n",
    "class_reports[\"augmented\"] \\\n",
    "    = get_test_results(model_augmented, val_dat, val_lbls)\n",
    "\n",
    "print(f\"Validation accuracy after augmentation: {acc_scores['augmented']}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrices['augmented'])\n",
    "print(class_reports['augmented'])\n",
    "\n",
    "augmented_log = read_csv(\"./training_logs/augmented_log.csv\")\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=[10, 7.5])\n",
    "plt.subplot(211)\n",
    "plt.xticks([0, 5, 10, 15, 20])\n",
    "plt.plot(augmented_log[\"loss\"], label=\"train loss\")\n",
    "plt.plot(augmented_log[\"val_loss\"], label=\"validation loss\")\n",
    "plt.title(\"Loss\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.xticks([0, 5, 10, 15, 20])\n",
    "plt.plot(augmented_log[\"acc\"], label=\"train accuracy\")\n",
    "plt.plot(augmented_log[\"val_acc\"], label=\"validation accuracy\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n",
    "\n",
    "# Free memory\n",
    "clear_session()\n",
    "collect()\n",
    "del model_augmented\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Cross-Model Comparison"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=[10, 7.5])\n",
    "plt.subplot(211)\n",
    "plt.xticks([0, 5, 10, 15, 20])\n",
    "plt.plot(unaugmented_log[\"val_loss\"], label=\"Before Augmentation\")\n",
    "plt.plot(augmented_log[\"val_loss\"], label=\"After Augmentation\")\n",
    "plt.title(\"Validation Loss\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.xticks([0, 5, 10, 15, 20])\n",
    "plt.plot(unaugmented_log[\"val_acc\"], label=\"VGG-11\")\n",
    "plt.plot(augmented_log[\"val_acc\"], label=\"VGG-13\")\n",
    "plt.title(\"Validation Accuracy\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.0, 1.0))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Conclusion\n",
    "\n",
    "# TO BE UPDATED\n",
    "\n",
    "From the above, we can observe that overfitting occurs in all models \n",
    "between the 5-10th epoch, and maximum validation accuracy is about 65%. \n",
    "However, this is before any regularisation measures are taken.\n",
    "\n",
    "Based on the cross-model analysis comparisons, it can be seen that \n",
    "VGG-16 and VGG-16(Conv1) achieved the lowest validation loss, and at \n",
    "those points, VGG-16(Conv1) achieved the higher validation accuracy.\n",
    "\n",
    "Therefore, we shall proceed to optimise the VGG-16(Conv1) model with\n",
    "hyperparameter tuning, followed by regularisation."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}